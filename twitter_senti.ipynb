{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import random\n",
    "\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tweepy in c:\\users\\91989\\anaconda3\\lib\\site-packages (3.8.0)\n",
      "Requirement already satisfied: six>=1.10.0 in c:\\users\\91989\\anaconda3\\lib\\site-packages (from tweepy) (1.12.0)\n",
      "Requirement already satisfied: requests>=2.11.1 in c:\\users\\91989\\anaconda3\\lib\\site-packages (from tweepy) (2.22.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\91989\\anaconda3\\lib\\site-packages (from tweepy) (1.3.0)\n",
      "Requirement already satisfied: PySocks>=1.5.7 in c:\\users\\91989\\anaconda3\\lib\\site-packages (from tweepy) (1.7.0)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\91989\\anaconda3\\lib\\site-packages (from requests>=2.11.1->tweepy) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\91989\\anaconda3\\lib\\site-packages (from requests>=2.11.1->tweepy) (1.24.2)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\users\\91989\\anaconda3\\lib\\site-packages (from requests>=2.11.1->tweepy) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\91989\\anaconda3\\lib\\site-packages (from requests>=2.11.1->tweepy) (2019.11.28)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\91989\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->tweepy) (3.1.0)\n",
      "Requirement already satisfied: textblob in c:\\users\\91989\\anaconda3\\lib\\site-packages (0.15.3)\n",
      "Requirement already satisfied: nltk>=3.1 in c:\\users\\91989\\anaconda3\\lib\\site-packages (from textblob) (3.4.4)\n",
      "Requirement already satisfied: six in c:\\users\\91989\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (1.12.0)\n",
      "Finished.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\91989\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\91989\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\91989\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\91989\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package conll2000 to\n",
      "[nltk_data]     C:\\Users\\91989\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package conll2000 is already up-to-date!\n",
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     C:\\Users\\91989\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "!pip install tweepy\n",
    "\n",
    "# TextBlob: textblob is the python library for processing textual data.\n",
    "\n",
    "# Install it using following pip command:\n",
    "!pip install textblob\n",
    "\n",
    "\n",
    "#Also, we need to install some corpora using following command:\n",
    "!python -m textblob.download_corpora\n",
    "\n",
    "#(Corpora is nothing but a large and structured set of texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "import tweepy \n",
    "from tweepy import OAuthHandler \n",
    "from textblob import TextBlob \n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "  \n",
    "class TwitterClient(object): \n",
    "    ''' \n",
    "    Generic Twitter Class for sentiment analysis. \n",
    "    We create a TwitterClient class. This class contains all the methods to interact with Twitter API and parsing tweets. \n",
    "    We use __init__ function to handle the authentication of API client.\n",
    "    \n",
    "    '''\n",
    "    def __init__(self): \n",
    "        \n",
    "        # __init__ is the constructor for a class. The self variable represents the instance of the object itself.\n",
    "        # When a class defines an __init__() method, class instantiation automatically invokes __init__() for the newly-created class instance. \n",
    "        \n",
    "        ''' \n",
    "        Class constructor or initialization method. \n",
    "        '''\n",
    "        # keys and tokens from the Twitter Dev Console \n",
    "        \n",
    "        consumer_key = 'I5EQI9SZPHbzHU1NY1KK3C2qx'\n",
    "        consumer_secret = 'OOsThzGqX1niE9YN6hPwoTG9Q4u5rij3mqsIuOwg9VsZve5LYR'\n",
    "        access_token = '147830710-w3n86zZ3f69z9xX8XfACEKAlCq8uRaFU7lFVGBM6'\n",
    "        access_token_secret = '9oTYADmKqvPXLH5pnO9tP3pBlN6mmAZEAbDn0kPAOznSD'\n",
    "        \n",
    "        # attempt authentication \n",
    "        try: \n",
    "            # create OAuthHandler object \n",
    "            self.auth = OAuthHandler(consumer_key, consumer_secret) \n",
    "            # set access token and secret \n",
    "            self.auth.set_access_token(access_token, access_token_secret) \n",
    "            # create tweepy API object to fetch tweets \n",
    "            self.api = tweepy.API(self.auth) \n",
    "\n",
    "        except: \n",
    "            print(\"Error: Authentication Failed\") \n",
    "  \n",
    "    def clean_tweet(self, tweet): \n",
    "        ''' \n",
    "        Utility function to clean tweet text by removing links, special characters \n",
    "        using simple regex statements. \n",
    "        '''\n",
    "        return(' '.join(re.sub(\"([,\\.():;!$%^&*\\d])|([^0-9A-Za-z \\t])\", \" \", tweet).split())) \n",
    "  \n",
    "    def get_tweet_sentiment(self, tweet): \n",
    "        ''' \n",
    "        Utility function to classify sentiment of passed tweet \n",
    "        using textblob's sentiment method \n",
    "        '''\n",
    "       \n",
    "        # create TextBlob object of passed tweet text \n",
    "        analysis = TextBlob(self.clean_tweet(tweet)) \n",
    "        # set sentiment \n",
    "        if analysis.sentiment.polarity > 0: \n",
    "            return('positive')\n",
    "        elif analysis.sentiment.polarity == 0: \n",
    "            return ('neutral')\n",
    "        else: \n",
    "            return ('negative')\n",
    "        \n",
    "\n",
    "\n",
    "  \n",
    "    def get_tweets(self, query, count = 100): \n",
    "        ''' \n",
    "        Main function to fetch tweets and parse them.\n",
    "        '''\n",
    "        # empty list to store parsed tweets \n",
    "        tweets = [] \n",
    "        \n",
    "        try: \n",
    "            # call twitter api to fetch tweets \n",
    "            fetched_tweets = self.api.search(q = query, count = count) \n",
    "              \n",
    "            # parsing tweets one by one \n",
    "            for tweet in fetched_tweets: \n",
    "                # empty dictionary to store required params of a tweet \n",
    "                parsed_tweet = {} \n",
    "  \n",
    "                # saving text of tweet \n",
    "                parsed_tweet['text'] = tweet.text\n",
    "                parsed_tweet['date'] = tweet.created_at\n",
    "                \n",
    "                # saving sentiment of tweet \n",
    "                parsed_tweet['sentiment'] = self.get_tweet_sentiment(tweet.text) \n",
    "  \n",
    "                # appending parsed tweet to tweets list \n",
    "                if tweet.retweet_count > 0: \n",
    "                    # if tweet has retweets, ensure that it is appended only once \n",
    "                    if parsed_tweet not in tweets: \n",
    "                        tweets.append(parsed_tweet) \n",
    "                else: \n",
    "                    tweets.append(parsed_tweet) \n",
    "  \n",
    "            # return parsed tweets \n",
    "            return (tweets) \n",
    "  \n",
    "        except tweepy.TweepError as e: \n",
    "            # print error (if any) \n",
    "            print(\"Error : \" + str(e)) \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(): \n",
    "    \n",
    "    # creating object of TwitterClient Class \n",
    "    api = TwitterClient() \n",
    "    \n",
    "    # calling function to get tweets \n",
    "    tweets = api.get_tweets(query = 'work from home covid', count = 1500) \n",
    "    \n",
    " \n",
    "    # picking positive tweets from tweets \n",
    "    ptweets = [tweet for tweet in tweets if tweet['sentiment'] == 'positive'] \n",
    "    \n",
    "    # percentage of positive tweets \n",
    "    print(\"Positive tweets percentage: {} %\".format(100*len(ptweets)/len(tweets))) \n",
    "    \n",
    "    # picking negative tweets from tweets \n",
    "    ntweets = [tweet for tweet in tweets if tweet['sentiment'] == 'negative'] \n",
    "    \n",
    "    # percentage of negative tweets \n",
    "    print(\"Negative tweets percentage: {} %\".format(100*len(ntweets)/len(tweets))) \n",
    "    \n",
    "    # percentage of neutral tweets \n",
    "    print(\"Neutral tweets percentage: {} % \".format(100*(len(tweets) - (len(ntweets) + len(ptweets)))/len(tweets))) \n",
    "  \n",
    "    # printing first 10 positive tweets \n",
    "    print(\"\\n\\nPositive tweets:\") \n",
    "    for tweet in ptweets[:10]: \n",
    "        print(tweet['text'])\n",
    "        # print(tweet['date'])\n",
    "  \n",
    "    # printing first 10 negative tweets \n",
    "    print(\"\\n\\nNegative tweets:\") \n",
    "    for tweet in ntweets[:10]: \n",
    "        print(tweet['text'])\n",
    "        # print(tweet['date'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive tweets percentage: 49.0 %\n",
      "Negative tweets percentage: 11.0 %\n",
      "Neutral tweets percentage: 40.0 % \n",
      "\n",
      "\n",
      "Positive tweets:\n",
      "üì£ New Podcast! \"Earth911 Podcast: Making environmental lemonade from COVID-19 lemons\" on @Spreaker #cfls #changes‚Ä¶ https://t.co/ThiWDgMlIw\n",
      "Wo rking from home is a solution for Small Businesses amid COVID-19 Social Distancing. Read more:‚Ä¶ https://t.co/H9xgYmikg5\n",
      "RT @niawag2011: Been thinking about this a lot. In a study in the Mission District in SF, no caucasians tested positive. Almost all positiv‚Ä¶\n",
      "@adamvaughan_uk Pre-covid would be appealing, post covid think I‚Äôll work from home far more often as will many I feel.\n",
      "RT @Rollits: Is your workplace \"covid-secure\"?  With the recent Government announcement encouraging those who can't work from home to go ba‚Ä¶\n",
      "RT @FarmersOfTheUK: I cover a lovely area of Yorkshire including the North Yorkshire Moors and Yorkshire Wolds. I‚Äôm home based so Covid has‚Ä¶\n",
      "RT @TaP_Arden: Join us on the 28th May at 3pm for a celebration of work from our graduating class of '20, from the comfort of your own home‚Ä¶\n",
      "RT @LeanneRowlands0: Writing my PhD thesis during Covid-19. I now really appreciate my level of focus pre-lockdown. But, in the grand schem‚Ä¶\n",
      "RT @pakhorse1: @AIMInvestor4 @Shan90aim fell from well over ¬£ and now lots of work on litigation from home and busy with covid consequences‚Ä¶\n",
      "RT @enisa_eu: üèòÔ∏è Securing smart homes and smart buildings from cyber security risks becomes more relevant than ever in\n",
      "times of #Covid19.‚Ä¶\n",
      "\n",
      "\n",
      "Negative tweets:\n",
      "The government has asked companies to work from home to stop this virus spreading and to stay indoors. Remote worki‚Ä¶ https://t.co/XzmmDsAT6g\n",
      "Zara: please work from home due to Covid-19\n",
      "\n",
      "Models: https://t.co/xdtdAqILj3\n",
      "@SkyNews We must learn to live with Covid.\n",
      "1/4 of our workforce is unable to work from home+has carried on:factory‚Ä¶ https://t.co/4RfJr77xwR\n",
      "@Chetangghule : As everyone is working from home due to current COVID-19 situation. It is mandatory that the power‚Ä¶ https://t.co/zlEjMER9ts\n",
      "RT @ErimusIB: COVID-19 related cyber-crime is on the rise. Cyber criminals are scanning for vulnerabilities in software and remote working‚Ä¶\n",
      "RT @AlmunierHassan: It's been 3 months now away from Family back in Abu Dhabi. We are stuck due to work related.\n",
      "Kids everyday ask when wil‚Ä¶\n",
      "RT @AlmunierHassan: It's been 3 months now away from Family back in Abu Dhabi. We are stuck due to work related.\n",
      "Kids everyday ask when wil‚Ä¶\n",
      "It's been 3 months now away from Family back in Abu Dhabi. We are stuck due to work related.\n",
      "Kids everyday ask when‚Ä¶ https://t.co/KHIOAqMehH\n",
      "The current climate has forced companies into making the transition from office to home, for some this change only‚Ä¶ https://t.co/JwlxMAisOK\n",
      "How can we work from home when @CityPowerJhb @Eskom_SA shuts us down at Woodmead its intolerable. Come on power pla‚Ä¶ https://t.co/VmVC2mZfn0\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\": \n",
    "    # calling main function \n",
    "    main() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<work.v.03: PosScore=0.125 NegScore=0.0>\n",
      "0.125\n",
      "0.0\n",
      "0.875\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import sentiwordnet as swn\n",
    "breakdown = swn.senti_synset('work.v.03')\n",
    "print(breakdown)\n",
    "print(breakdown.pos_score())\n",
    "print(breakdown.neg_score())\n",
    "print(breakdown.obj_score())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
